- data.train_files=/data/RLAMG/data/openr1_multi_longcot_pr.parquet
- data.val_files=/data/RLAMG/data/valid_math_all.parquet
- data.prompt_key=prompt
- data.truncation=right
- data.max_prompt_length=1024
- data.max_response_length=8192
- data.train_batch_size=32
- data.val_batch_size=32
- data.shuffle=True
- data.num_off_policy_targets=2
- data.max_available_targets=4
- data.reward_impl_version=5
- data.val_reward_impl_version=5
- actor_rollout_ref.rollout.n=8
- algorithm.adv_estimator=grpo
- algorithm.use_kl_in_reward=False
- algorithm.kl_ctrl.kl_coef=0.0
- algorithm.norm_adv_by_std_in_grpo=False
- actor_rollout_ref.actor.use_kl_loss=False
- actor_rollout_ref.actor.kl_loss_coef=0.0
- actor_rollout_ref.actor.clip_ratio_low=0.2
- actor_rollout_ref.actor.clip_ratio_high=0.2
- actor_rollout_ref.actor.clip_ratio_c=10.0
- actor_rollout_ref.model.use_remove_padding=True
- +actor_rollout_ref.model.override_config.max_position_embeddings=32768
- actor_rollout_ref.actor.use_dynamic_bsz=True
- actor_rollout_ref.ref.log_prob_use_dynamic_bsz=True
- actor_rollout_ref.rollout.log_prob_use_dynamic_bsz=True
- actor_rollout_ref.actor.ppo_max_token_len_per_gpu=9316
- actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=18432
- actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=18432
- actor_rollout_ref.rollout.name=vllm
- actor_rollout_ref.model.path=/data1/models/Qwen2.5-1.5B-Instruct
- actor_rollout_ref.model.enable_gradient_checkpointing=True
- actor_rollout_ref.actor.optim.lr=1e-6
- actor_rollout_ref.actor.optim.lr_warmup_steps=0
- actor_rollout_ref.actor.optim.weight_decay=0.1
- actor_rollout_ref.actor.ppo_mini_batch_size=8
- actor_rollout_ref.actor.fsdp_config.param_offload=True
- actor_rollout_ref.actor.fsdp_config.optimizer_offload=True
- actor_rollout_ref.actor.entropy_coeff=0.001
- actor_rollout_ref.actor.grad_clip=1.0
- actor_rollout_ref.actor.loss_agg_mode=token-mean
- actor_rollout_ref.actor.ulysses_sequence_parallel_size=1
- actor_rollout_ref.rollout.gpu_memory_utilization=0.80
- actor_rollout_ref.rollout.tensor_model_parallel_size=1
- actor_rollout_ref.rollout.enable_chunked_prefill=True
- actor_rollout_ref.rollout.max_num_batched_tokens=9216
- actor_rollout_ref.rollout.temperature=1.0
- actor_rollout_ref.rollout.top_p=1.0
- actor_rollout_ref.rollout.top_k=-1
- actor_rollout_ref.rollout.val_kwargs.temperature=0.6
- actor_rollout_ref.rollout.val_kwargs.top_p=0.7
- actor_rollout_ref.rollout.val_kwargs.top_k=-1
- actor_rollout_ref.rollout.val_kwargs.do_sample=True
- actor_rollout_ref.rollout.val_kwargs.n=1
- actor_rollout_ref.ref.fsdp_config.param_offload=True
- actor_rollout_ref.ref.ulysses_sequence_parallel_size=1
- actor_rollout_ref.actor.fsdp_config.fsdp_size=8
- actor_rollout_ref.rollout.max_prefix_len=8192
- actor_rollout_ref.rollout.prefix_share_across_samples=False
- actor_rollout_ref.rollout.prefix_strategy=random
- actor_rollout_ref.rollout.min_prefix_ratio=1.0
- actor_rollout_ref.rollout.max_prefix_ratio=1.0
- actor_rollout_ref.rollout.prefix_reward_weight_alpha=1.0
- actor_rollout_ref.ref.use_ref=False
- actor_rollout_ref.actor.use_sft_multitask_loss=False
- actor_rollout_ref.actor.use_off_policy_loss=True
- actor_rollout_ref.actor.off_policy_normalize=False
- actor_rollout_ref.actor.off_policy_reshape=p_div_p_0.1
- actor_rollout_ref.actor.off_policy_loss_impl=token
- actor_rollout_ref.actor.loss_remove_token_mean=True
- actor_rollout_ref.actor.loss_remove_clip=False
- actor_rollout_ref.rollout.injection_strategy=adaptive
- actor_rollout_ref.rollout.target_selection_strategy=best_k
- reward_model.reward_manager=naive
- +reward_model.reward_kwargs.overlong_buffer_cfg.enable=True
- +reward_model.reward_kwargs.overlong_buffer_cfg.len=4096
- +reward_model.reward_kwargs.overlong_buffer_cfg.penalty_factor=1.0
- +reward_model.reward_kwargs.overlong_buffer_cfg.log=False
- +reward_model.reward_kwargs.max_resp_len=8192
- reward_model.reward_manager_shaping_function_name=threshold_0
- reward_model.compute_score_name=mean_exp_log_softmax
- reward_model.repetition_penalty=True
- reward_model.off_policy_reward_manager=naive
- reward_model.val_reward_manager=naive
- reward_model.format_mode=R1
- reward_model.format_coefficient=0.0
- trainer.logger=["console","wandb"]
- trainer.project_name=MIX-DAPO
- trainer.experiment_name=MIX-DAPO-Qwen2.5-1.5b-adaptive-2longcot
- trainer.n_gpus_per_node=1
- trainer.nnodes=1
- trainer.val_before_train=False
- trainer.test_freq=10
- trainer.save_freq=20
- trainer.total_epochs=10
- trainer.total_training_steps=210
- +trainer.max_optim_to_keep=2
- trainer.default_local_dir=/data1/RL_output/Qwen2.5-1.5B/checkpoints/MIX-DAPO/MIX-DAPO-Qwen2.5-1.5b-adaptive-2longcot
- trainer.default_hdfs_dir=null
- trainer.resume_mode=auto
- trainer.log_val_generations=10
